{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66c8b3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTIVE\n",
      "False\n",
      "                            close    high     low  trade_count    open  \\\n",
      "timestamp                                                                \n",
      "2025-11-24 09:00:00+00:00  272.19  272.50  272.19           92  272.50   \n",
      "2025-11-24 09:01:00+00:00  272.26  272.26  272.26           57  272.26   \n",
      "2025-11-24 09:02:00+00:00  272.48  272.48  272.28           52  272.28   \n",
      "\n",
      "                           volume        vwap  \n",
      "timestamp                                      \n",
      "2025-11-24 09:00:00+00:00    1357  272.396247  \n",
      "2025-11-24 09:01:00+00:00     573  272.260000  \n",
      "2025-11-24 09:02:00+00:00    1371  272.382745  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import alpaca_trade_api as tradeapi\n",
    "\n",
    "\n",
    "BASE_URL = os.getenv(\"ALPACA_BASE_URL\", \"https://paper-api.alpaca.markets\")\n",
    "API_KEY = os.getenv(\"ALPACA_API_KEY\", \"PKGWUROP3UUI5Q4TISVTRSBHZ6\")\n",
    "API_SECRET = os.getenv(\"ALPACA_API_SECRET\", \"FwhsDd2fUn4UQ2EjLtGXu5L8PM2dRTmFf2i69M8CMHHW\")\n",
    "api = tradeapi.REST(API_KEY, API_SECRET, BASE_URL)\n",
    "print(api.get_account().status)      # expect 'ACTIVE'\n",
    "print(api.get_clock().is_open)       # True/False\n",
    "print(api.get_bars('AAPL', '1Min', limit=3).df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9800c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max abs error: 2.842170943040401e-14\n",
      "max provable bound: 0.0\n",
      "bound respected everywhere?: True\n",
      "sum of bounds (proxy for skipped contrib): 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def egamm(A, B, br=32, bc=32, bk=32, eps=1e-3, return_bounds=True):\n",
    "    \"\"\"\n",
    "    Envelope-Guarded Approximate MatMul (EGAMM).\n",
    "    \n",
    "    Skips inner-block tile products when envelope bound U < eps.\n",
    "\n",
    "    Returns:\n",
    "      C_hat: approximate A @ B\n",
    "      bound: per-entry provable absolute error bound (if return_bounds=True)\n",
    "    \"\"\"\n",
    "    A = np.asarray(A)\n",
    "    B = np.asarray(B)\n",
    "    m, k = A.shape\n",
    "    k2, n = B.shape\n",
    "    assert k == k2, \"Inner dimensions must match\"\n",
    "\n",
    "    C_hat = np.zeros((m, n), dtype=np.result_type(A, B))\n",
    "    bound = np.zeros((m, n), dtype=np.float64) if return_bounds else None\n",
    "\n",
    "    # Tile over rows and cols\n",
    "    for r0 in range(0, m, br):\n",
    "        r1 = min(r0 + br, m)\n",
    "        for c0 in range(0, n, bc):\n",
    "            c1 = min(c0 + bc, n)\n",
    "\n",
    "            # Sum over inner blocks\n",
    "            for t0 in range(0, k, bk):\n",
    "                t1 = min(t0 + bk, k)\n",
    "\n",
    "                A_blk = A[r0:r1, t0:t1]      # shape (<=br, <=bk)\n",
    "                B_blk = B[t0:t1, c0:c1]      # shape (<=bk, <=bc)\n",
    "\n",
    "                # Envelopes\n",
    "                alpha = np.max(np.abs(A_blk), axis=0)  # length bk'\n",
    "                beta  = np.max(np.abs(B_blk), axis=1)  # length bk'\n",
    "\n",
    "                # Bound on every entry of this tile contribution\n",
    "                U = float(alpha @ beta)\n",
    "\n",
    "                if U < eps:\n",
    "                    if return_bounds:\n",
    "                        bound[r0:r1, c0:c1] += U\n",
    "                    continue\n",
    "\n",
    "                # Otherwise compute this block contribution exactly\n",
    "                C_hat[r0:r1, c0:c1] += A_blk @ B_blk\n",
    "\n",
    "    return (C_hat, bound) if return_bounds else (C_hat, None)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Small demo to validate the bound\n",
    "    rng = np.random.default_rng(0)\n",
    "    m, k, n = 96, 128, 80\n",
    "    A = rng.normal(size=(m, k))\n",
    "    B = rng.normal(size=(k, n))\n",
    "\n",
    "    C_exact = A @ B\n",
    "    C_hat, bound = egamm(A, B, br=24, bc=20, bk=16, eps=0.05)\n",
    "\n",
    "    err = np.abs(C_exact - C_hat)\n",
    "\n",
    "    print(\"max abs error:\", err.max())\n",
    "    print(\"max provable bound:\", bound.max())\n",
    "    print(\"bound respected everywhere?:\", np.all(err <= bound + 1e-12))\n",
    "\n",
    "    # How much work was skipped?\n",
    "    skipped_mass = np.sum(bound)  # crude proxy\n",
    "    print(\"sum of bounds (proxy for skipped contrib):\", skipped_mass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EGAMMLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Envelope-Guarded Approximate MatMul linear layer.\n",
    "    CPU proof-of-concept. Safe skipping via max-abs envelopes.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True,\n",
    "                 br=64, bc=64, bk=64, eps=1e-3):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.br, self.bc, self.bk = br, bc, bk\n",
    "        self.eps = eps\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "        # stats\n",
    "        self.last_skip_ratio = 0.0\n",
    "        self.last_max_bound = 0.0\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _egamm_matmul(self, A, B):\n",
    "        \"\"\"\n",
    "        A: (m, k)  activations\n",
    "        B: (k, n)  weight^T\n",
    "        returns C_hat (m, n), bound (m, n), skip_ratio\n",
    "        \"\"\"\n",
    "        m, k = A.shape\n",
    "        k2, n = B.shape\n",
    "        assert k == k2\n",
    "\n",
    "        br, bc, bk, eps = self.br, self.bc, self.bk, self.eps\n",
    "\n",
    "        C_hat = A.new_zeros((m, n))\n",
    "        bound = A.new_zeros((m, n), dtype=torch.float32)\n",
    "\n",
    "        total_blocks = 0\n",
    "        skipped_blocks = 0\n",
    "        maxU = 0.0\n",
    "\n",
    "        for r0 in range(0, m, br):\n",
    "            r1 = min(r0 + br, m)\n",
    "            A_r = A[r0:r1, :]\n",
    "\n",
    "            for c0 in range(0, n, bc):\n",
    "                c1 = min(c0 + bc, n)\n",
    "\n",
    "                for t0 in range(0, k, bk):\n",
    "                    t1 = min(t0 + bk, k)\n",
    "\n",
    "                    A_blk = A_r[:, t0:t1]      # (<=br, <=bk')\n",
    "                    B_blk = B[t0:t1, c0:c1]    # (<=bk', <=bc)\n",
    "\n",
    "                    # envelopes\n",
    "                    alpha = A_blk.abs().max(dim=0).values   # (bk',)\n",
    "                    beta  = B_blk.abs().max(dim=1).values  # (bk',)\n",
    "\n",
    "                    U = float(alpha @ beta)\n",
    "                    total_blocks += 1\n",
    "                    maxU = max(maxU, U)\n",
    "\n",
    "                    if U < eps:\n",
    "                        skipped_blocks += 1\n",
    "                        bound[r0:r1, c0:c1] += U\n",
    "                        continue\n",
    "\n",
    "                    C_hat[r0:r1, c0:c1] += A_blk @ B_blk\n",
    "\n",
    "        skip_ratio = skipped_blocks / max(total_blocks, 1)\n",
    "        return C_hat, bound, skip_ratio, maxU\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, in_features)\n",
    "        output: (batch, out_features)\n",
    "        \"\"\"\n",
    "        # Use no_grad for the pruning+matmul, then attach gradients\n",
    "        # via a straight-through estimator on the skipped blocks.\n",
    "        # (Keeps layer trainable; we're benchmarking speed)\n",
    "        with torch.no_grad():\n",
    "            C_hat, bound, skip_ratio, maxU = self._egamm_matmul(\n",
    "                x, self.weight.t()\n",
    "            )\n",
    "\n",
    "        # Straight-through: treat C_hat as if exact for backward\n",
    "        y = x @ self.weight.t()\n",
    "        y = y + (C_hat - y).detach()\n",
    "\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "\n",
    "        # log stats\n",
    "        self.last_skip_ratio = skip_ratio\n",
    "        self.last_max_bound = maxU\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9d435",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
